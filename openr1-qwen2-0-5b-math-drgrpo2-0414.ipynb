{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":233234369,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# OpenR1 Qwen2-0.5B-math-drgrpo2","metadata":{}},{"cell_type":"markdown","source":"- gpu: T4*2\n- model: Qwen/Qwen2-0.5B\n- data: stpete2/openr1-math-part\n- method: drgrpo\n- output: Qwen2-0.5B-math-drgrpo\n  ","metadata":{}},{"cell_type":"markdown","source":"The previous approach is correct but complicated\n\nhttps://www.kaggle.com/code/stpeteishii/openr1-qwen2-0-5b-math-drgrpo-0414","metadata":{}},{"cell_type":"markdown","source":"## Open-R1 \nis an open initiative to replicate and extend the techniques behind DeepSeek-R1, a state-of-the-art reasoning model, in a fully transparent and collaborative way: \n\nhttps://github.com/huggingface/open-r1\n\n","metadata":{}},{"cell_type":"markdown","source":"By selecting the model, dataset, and method, and running the training command from the command line, we were able to successfully perform training using the OpenR1 environment.\n\nCconsidering the limitations of the notebook environment, I limited the model and data to a minimum. And the following techniques are used. \n\n* 1. Using LoRA (Low-Rank Adaptation)\n* 2. Gradient checkpointing\n* 3. Batching optimizations\n* 4. BF16 mixed precision\n* 5. Sequence length limit\n* 6. Data packing\n\nThis setting is far from sufficient for effective training, but on the other hand, it allows us to check the operation of the method in a short time.\n\nThis minimal configuration allows for rapid validation of the training pipeline even with limited resources, and is a useful starting point before scaling up to larger experiments.","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=secret_value)\n\n# save metrics into wandb folder\nimport os\nos.environ[\"WANDB_DIR\"] = \"./wandb\"\nwandb.init(project=\"250414dr2\", mode=\"online\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/huggingface/open-r1.git\n!pip install -e ./open-r1\n!pip show open-r1\n!pip install trl --upgrade","metadata":{"_uuid":"56441817-9a26-48a8-9e2a-20d0519b1368","_cell_guid":"c938dd8b-8728-417e-b2b8-e5ea9a6531cb","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nos.chdir('./open-r1')","metadata":{"_uuid":"fe72f3f4-3723-48ec-9cc6-fc79a67f773a","_cell_guid":"b7f206d2-971a-4156-a2da-e13c3ec6a849","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation\n#!pip install vllm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_content = \"\"\"\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_clipping: 1.0\n  zero3_init_flag: true\n  zero_stage: 1\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n\"\"\"\n\nconfig_path = \"custom_config.yaml\"\nPath(config_path).write_text(config_content)\n\n\n#################################\n\n\nconfig_content2 = \"\"\"\n# Model arguments\nmodel_name_or_path: Qwen/Qwen2-0.5B\nmodel_revision: main\ntorch_dtype: bfloat16\nattn_implementation: eager\n\n# Data training arguments\ndataset_name: stpete2/openr1-math-part\ndataset_prompt_column: problem\nsystem_prompt: |\n  You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n  ...\n  </think>\n  <answer>\n  ...\n  </answer>\n# GRPO trainer config\nbf16: true\nuse_vllm: false\ndo_eval: false\ngradient_accumulation_steps: 2\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nhub_model_id: Qwen2-0.5B-math-drgrpo\nhub_strategy: every_save\nlearning_rate: 2.0e-05\nlog_completions: true\nlog_level: info\nlogging_first_step: true\nlogging_steps: 1\nlogging_strategy: steps\nlr_scheduler_type: cosine\nmax_prompt_length: 256\nmax_completion_length: 512\nmax_steps: -1\nnum_generations: 4\n\nscale_rewards: false\n\nnum_train_epochs: 1\noutput_dir: data/Qwen2-0.5B-math-drgrpo\noverwrite_output_dir: true\nper_device_eval_batch_size: 16\nper_device_train_batch_size: 8\npush_to_hub: false\nreport_to:\n- wandb\nreward_funcs:\n- accuracy\n- format\n- tag_count\nreward_weights:\n- 1.0\n- 1.0\n- 1.0\nsave_strategy: \"epoch\"\nsave_total_limit: 1\nseed: 42\nwarmup_ratio: 0.1\n\"\"\"\n\nconfig_path2 = \"custom_config2.yaml\"\nPath(config_path2).write_text(config_content2)\n\n\n##########################################################\n#!accelerate launch --config_file custom_config.yaml grpo2.py \\\n\n\n!accelerate launch --config_file custom_config.yaml src/open_r1/grpo.py \\\n--config custom_config2.yaml \\\n--disable_tqdm=False\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}