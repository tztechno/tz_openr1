{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# OpenR1 Qwen2.5-0.5B-gsm8k-cppo\n* num_generations: 8\n* num_train_epochs: 1","metadata":{"id":"0UCjHIv3qInj"}},{"cell_type":"markdown","source":"https://github.com/lzhxmu/CPPO.git\n","metadata":{"id":"CgmTXOMhqInp"}},{"cell_type":"markdown","source":"- gpu: L4*1(Colab)\n- model: Qwen/Qwen2.5-0.5B\n- data: stpete2/openai-gsm8k-part-100\n- method: cppo\n- output: Qwen2.5-0.5B-gsm8k-cppo","metadata":{"id":"6zTQ7bNCqInq"}},{"cell_type":"markdown","source":"###### unique setting for cppo in custom_config2.yaml\n- metric: smallest\n- pruning: 0.5\n- allocation: true","metadata":{"id":"aXnW58qOqInr"}},{"cell_type":"markdown","source":"## Open-R1\nis an open initiative to replicate and extend the techniques behind DeepSeek-R1, a state-of-the-art reasoning model, in a fully transparent and collaborative way:\n\nhttps://github.com/huggingface/open-r1\n\n","metadata":{"id":"f_cvz_UxqInr"}},{"cell_type":"markdown","source":"By selecting the model, dataset, and method, and running the training command from the command line, we were able to successfully perform training using the OpenR1 environment.\n\nCconsidering the limitations of the notebook environment, I limited the model and data to a minimum. And the following techniques are used.\n\n* 1. Using LoRA (Low-Rank Adaptation)\n* 2. Gradient checkpointing\n* 3. Batching optimizations\n* 4. BF16 mixed precision\n* 5. Sequence length limit\n* 6. Data packing\n\nThis setting is far from sufficient for effective training, but on the other hand, it allows us to check the operation of the method in a short time.\n\nThis minimal configuration allows for rapid validation of the training pipeline even with limited resources, and is a useful starting point before scaling up to larger experiments.","metadata":{"id":"y6T6czYIqIns"}},{"cell_type":"markdown","source":"\n","metadata":{"id":"013IfWRwqIns"}},{"cell_type":"code","source":"import wandb\nsecret_value = \"xxxxx\"\nwandb.login(key=secret_value)\n\n# save metrics into wandb folder\nimport os\nos.environ[\"WANDB_DIR\"] = \"./wandb\"\nwandb.init(project=\"250424cppo\", mode=\"online\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:37:51.031315Z","iopub.execute_input":"2025-04-13T16:37:51.031506Z","iopub.status.idle":"2025-04-13T16:37:51.335575Z","shell.execute_reply.started":"2025-04-13T16:37:51.031486Z","shell.execute_reply":"2025-04-13T16:37:51.334959Z"},"id":"_lO5msMsqInt","colab":{"base_uri":"https://localhost:8080/","height":247},"outputId":"5d2ded5e-6fb6-4c57-c68d-9a5b3fd48544"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstpeteishii\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.10"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250507_030520-rz1mjdud</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/stpeteishii/250424cppo/runs/rz1mjdud' target=\"_blank\">deep-leaf-4</a></strong> to <a href='https://wandb.ai/stpeteishii/250424cppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/stpeteishii/250424cppo' target=\"_blank\">https://wandb.ai/stpeteishii/250424cppo</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/stpeteishii/250424cppo/runs/rz1mjdud' target=\"_blank\">https://wandb.ai/stpeteishii/250424cppo/runs/rz1mjdud</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/stpeteishii/250424cppo/runs/rz1mjdud?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fb379d12150>"]},"metadata":{},"execution_count":1}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/lzhxmu/CPPO.git\n!pip install -e ./CPPO\n!pip show CPPO","metadata":{"_uuid":"56441817-9a26-48a8-9e2a-20d0519b1368","_cell_guid":"c938dd8b-8728-417e-b2b8-e5ea9a6531cb","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:37:51.336307Z","iopub.execute_input":"2025-04-13T16:37:51.336538Z","iopub.status.idle":"2025-04-13T16:38:11.319488Z","shell.execute_reply.started":"2025-04-13T16:37:51.336518Z","shell.execute_reply":"2025-04-13T16:38:11.31866Z"},"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"id":"uTiDJDJJqInw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ba412ae-2c89-4ce2-bd7d-a818c6fc38ff","collapsed":false},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CPPO'...\n","remote: Enumerating objects: 258, done.\u001b[K\n","remote: Counting objects: 100% (74/74), done.\u001b[K\n","remote: Compressing objects: 100% (33/33), done.\u001b[K\n","remote: Total 258 (delta 48), reused 58 (delta 39), pack-reused 184 (from 1)\u001b[K\n","Receiving objects: 100% (258/258), 4.15 MiB | 8.77 MiB/s, done.\n","Resolving deltas: 100% (113/113), done.\n","Obtaining file:///content/CPPO\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting trl@ git+https://github.com/huggingface/trl.git@69ad852e5654a77f1695eb4c608906fe0c7e8624 (from open-r1==0.1.0.dev0)\n","  Cloning https://github.com/huggingface/trl.git (to revision 69ad852e5654a77f1695eb4c608906fe0c7e8624) to /tmp/pip-install-igq2oila/trl_d202c3dc9f9348528d4698a20240a240\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-install-igq2oila/trl_d202c3dc9f9348528d4698a20240a240\n","  Running command git rev-parse -q --verify 'sha^69ad852e5654a77f1695eb4c608906fe0c7e8624'\n","  Running command git fetch -q https://github.com/huggingface/trl.git 69ad852e5654a77f1695eb4c608906fe0c7e8624\n","  Running command git checkout -q 69ad852e5654a77f1695eb4c608906fe0c7e8624\n","  Resolved https://github.com/huggingface/trl.git to commit 69ad852e5654a77f1695eb4c608906fe0c7e8624\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate==1.4.0 (from open-r1==0.1.0.dev0)\n","  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n","Collecting bitsandbytes>=0.43.0 (from open-r1==0.1.0.dev0)\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from open-r1==0.1.0.dev0) (0.8.1)\n","Collecting datasets>=3.2.0 (from open-r1==0.1.0.dev0)\n","  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n","Collecting deepspeed==0.15.4 (from open-r1==0.1.0.dev0)\n","  Downloading deepspeed-0.15.4.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting hf_transfer>=0.1.4 (from open-r1==0.1.0.dev0)\n","  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0) (0.30.2)\n","Collecting langdetect (from open-r1==0.1.0.dev0)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting latex2sympy2_extended>=1.0.6 (from open-r1==0.1.0.dev0)\n","  Downloading latex2sympy2_extended-1.10.1-py3-none-any.whl.metadata (5.3 kB)\n","Collecting math-verify==0.5.2 (from open-r1==0.1.0.dev0)\n","  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n","Collecting liger_kernel==0.5.3 (from open-r1==0.1.0.dev0)\n","  Downloading liger_kernel-0.5.3-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from open-r1==0.1.0.dev0) (24.2)\n","Requirement already satisfied: safetensors>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from open-r1==0.1.0.dev0) (0.5.3)\n","Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.11/dist-packages (from open-r1==0.1.0.dev0) (0.2.0)\n","Collecting transformers==4.49.0 (from open-r1==0.1.0.dev0)\n","  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wandb>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from open-r1==0.1.0.dev0) (0.19.10)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.4.0->open-r1==0.1.0.dev0) (2.0.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.4.0->open-r1==0.1.0.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.4.0->open-r1==0.1.0.dev0) (6.0.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.4.0->open-r1==0.1.0.dev0) (2.6.0+cu124)\n","Collecting hjson (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n","  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.4->open-r1==0.1.0.dev0) (1.1.0)\n","Collecting ninja (from deepspeed==0.15.4->open-r1==0.1.0.dev0)\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.4->open-r1==0.1.0.dev0) (9.0.0)\n","Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.4->open-r1==0.1.0.dev0) (2.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.4->open-r1==0.1.0.dev0) (4.67.1)\n","Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.15.4->open-r1==0.1.0.dev0) (12.570.86)\n","Requirement already satisfied: triton>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from liger_kernel==0.5.3->open-r1==0.1.0.dev0) (3.2.0)\n","Collecting latex2sympy2_extended>=1.0.6 (from open-r1==0.1.0.dev0)\n","  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n","Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended>=1.0.6->open-r1==0.1.0.dev0)\n","  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from latex2sympy2_extended>=1.0.6->open-r1==0.1.0.dev0) (1.13.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->open-r1==0.1.0.dev0) (3.18.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->open-r1==0.1.0.dev0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->open-r1==0.1.0.dev0) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->open-r1==0.1.0.dev0) (0.21.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->open-r1==0.1.0.dev0) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->open-r1==0.1.0.dev0) (2.2.2)\n","Collecting xxhash (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=3.2.0->open-r1==0.1.0.dev0)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.2.0->open-r1==0.1.0.dev0)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->open-r1==0.1.0.dev0) (3.11.15)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.2->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0) (4.13.2)\n","Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n","  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n","Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0)\n","  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0) (3.0.51)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (8.1.8)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (4.3.7)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (5.29.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (2.27.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (1.3.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.19.1->open-r1==0.1.0.dev0) (75.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect->open-r1==0.1.0.dev0) (1.17.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl@ git+https://github.com/huggingface/trl.git@69ad852e5654a77f1695eb4c608906fe0c7e8624->open-r1==0.1.0.dev0) (13.9.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->open-r1==0.1.0.dev0) (1.20.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.19.1->open-r1==0.1.0.dev0) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed==0.15.4->open-r1==0.1.0.dev0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed==0.15.4->open-r1==0.1.0.dev0) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed==0.15.4->open-r1==0.1.0.dev0) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0->open-r1==0.1.0.dev0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0->open-r1==0.1.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0->open-r1==0.1.0.dev0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0->open-r1==0.1.0.dev0) (2025.4.26)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->latex2sympy2_extended>=1.0.6->open-r1==0.1.0.dev0) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->open-r1==0.1.0.dev0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->open-r1==0.1.0.dev0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->open-r1==0.1.0.dev0) (2025.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl@ git+https://github.com/huggingface/trl.git@69ad852e5654a77f1695eb4c608906fe0c7e8624->open-r1==0.1.0.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl@ git+https://github.com/huggingface/trl.git@69ad852e5654a77f1695eb4c608906fe0c7e8624->open-r1==0.1.0.dev0) (2.19.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.19.1->open-r1==0.1.0.dev0) (5.0.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl@ git+https://github.com/huggingface/trl.git@69ad852e5654a77f1695eb4c608906fe0c7e8624->open-r1==0.1.0.dev0) (0.1.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]<1.0,>=0.19.2->open-r1==0.1.0.dev0) (0.2.13)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.4.0->open-r1==0.1.0.dev0) (3.0.2)\n","Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading liger_kernel-0.5.3-py3-none-any.whl (113 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n","Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m138.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n","Building wheels for collected packages: deepspeed, langdetect, trl\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.15.4-py3-none-any.whl size=1527833 sha256=9710e48010eac9f9b1609ea063cf97ece50b3543206e5ef2f42ad18f7dc35775\n","  Stored in directory: /root/.cache/pip/wheels/9f/b7/07/035dfeaae31b5766822083a749891e45aab9c72d25a78b7dd0\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3fda4088740bdd18869a3155d3b720901aaf1ba98fc3ccb2782c90cf01c282ca\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for trl: filename=trl-0.16.0.dev0-py3-none-any.whl size=323018 sha256=4d7da257c3199d2c8267f5b734704639532912ef4c8d1c306726750c84ccfd40\n","  Stored in directory: /root/.cache/pip/wheels/08/24/1f/6c6247d1a09ee74d1009cb5c5bd1e4be5c9ec09aa9afa6b806\n","Successfully built deepspeed langdetect trl\n","Installing collected packages: hjson, antlr4-python3-runtime, xxhash, pfzy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, langdetect, hf_transfer, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, latex2sympy2_extended, InquirerPy, nvidia-cusolver-cu12, math-verify, transformers, datasets, liger_kernel, deepspeed, bitsandbytes, accelerate, trl, open-r1\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.51.3\n","    Uninstalling transformers-4.51.3:\n","      Successfully uninstalled transformers-4.51.3\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 1.6.0\n","    Uninstalling accelerate-1.6.0:\n","      Successfully uninstalled accelerate-1.6.0\n","  Running setup.py develop for open-r1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed InquirerPy-0.3.4 accelerate-1.4.0 antlr4-python3-runtime-4.13.2 bitsandbytes-0.45.5 datasets-3.5.1 deepspeed-0.15.4 dill-0.3.8 fsspec-2025.3.0 hf_transfer-0.1.9 hjson-3.1.0 langdetect-1.0.9 latex2sympy2_extended-1.0.6 liger_kernel-0.5.3 math-verify-0.5.2 multiprocess-0.70.16 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 open-r1-0.1.0.dev0 pfzy-0.3.4 transformers-4.49.0 trl-0.16.0.dev0 xxhash-3.5.0\n","\u001b[33mWARNING: Package(s) not found: CPPO\u001b[0m\u001b[33m\n","\u001b[0m"]}],"execution_count":2},{"cell_type":"code","source":"import os\nos.chdir('./CPPO')","metadata":{"_uuid":"fe72f3f4-3723-48ec-9cc6-fc79a67f773a","_cell_guid":"b7f206d2-971a-4156-a2da-e13c3ec6a849","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:38:11.320503Z","iopub.execute_input":"2025-04-13T16:38:11.320758Z","iopub.status.idle":"2025-04-13T16:38:11.325518Z","shell.execute_reply.started":"2025-04-13T16:38:11.320725Z","shell.execute_reply":"2025-04-13T16:38:11.324694Z"},"jupyter":{"outputs_hidden":false},"id":"jTilLUVCqInx","collapsed":false},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:38:11.326368Z","iopub.execute_input":"2025-04-13T16:38:11.32661Z","iopub.status.idle":"2025-04-13T16:38:11.45375Z","shell.execute_reply.started":"2025-04-13T16:38:11.326589Z","shell.execute_reply":"2025-04-13T16:38:11.452837Z"},"id":"0XsDa1lFqIny","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f84ac6bd-2a6d-40b0-ae45-ba729a5697b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["LICENSE  Makefile  README.md  recipes  scripts\tsetup.cfg  setup.py  src\n"]}],"execution_count":4},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation\n#!pip install vllm","metadata":{"trusted":true,"id":"SfOVzqx3qInz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92f80f27-db81-4a3d-be08-1b6143298c05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flash-attn\n","  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/6.0 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n","  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n","Successfully built flash-attn\n","Installing collected packages: flash-attn\n","Successfully installed flash-attn-2.7.4.post1\n"]}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"id":"dgj6v5MS5hR5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\n\nconfig_content = \"\"\"\ncompute_environment: LOCAL_MACHINE\ndebug: false\n\ndeepspeed_config:\n  gradient_clipping: 1.0\n  zero3_init_flag: true\n  zero_stage: 1\ndistributed_type: no\n\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n\"\"\"\n\nconfig_path = \"custom_config.yaml\"\nPath(config_path).write_text(config_content)\n\n\n#################################\n\n\nconfig_content2 = \"\"\"\n# Model arguments\nmodel_name_or_path: stpete2/Qwen2.5-0.5B-gsm8k-sft\nmodel_revision: main\ntorch_dtype: bfloat16\nattn_implementation: eager\n\n# Data training arguments\ndataset_name: stpete2/openai-gsm8k-part\nsystem_prompt: |\n  You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n  ...\n  </think>\n  <answer>\n  ...\n  </answer>\n# GRPO trainer config\nbf16: true\nuse_vllm: false\ndo_eval: false\ngradient_accumulation_steps: 2\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nhub_model_id: Qwen2.5-0.5B-gsm8k-cppo\nhub_strategy: every_save\nlearning_rate: 2.0e-06\nlog_completions: true\nlog_level: info\nlogging_first_step: true\nlogging_steps: 1\nlogging_strategy: steps\nlr_scheduler_type: cosine\nmax_prompt_length: 256\nmax_completion_length: 512\nmax_steps: -1\n\nnum_generations: 8\nnum_train_epochs: 1\n\nmetric: smallest\npruning: 0.75\nallocation: true\n\noutput_dir: data/Qwen2.5-0.5B-gsm8k-cppo\noverwrite_output_dir: true\nper_device_eval_batch_size: 16\nper_device_train_batch_size: 8\npush_to_hub: false\nreport_to:\n- wandb\nreward_funcs:\n- accuracy\n- format\n- tag_count\nreward_weights:\n- 1.0\n- 1.0\n- 1.0\nsave_strategy: \"epoch\"\nsave_total_limit: 1\nseed: 42\nwarmup_ratio: 0.1\n\"\"\"\n\nconfig_path2 = \"custom_config2.yaml\"\nPath(config_path2).write_text(config_content2)\n\n\n##########################################################\n\n\n!accelerate launch --config_file custom_config.yaml src/open_r1/grpo_gsm.py \\\n--config custom_config2.yaml \\\n--disable_tqdm=False\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:38:11.454847Z","iopub.execute_input":"2025-04-13T16:38:11.455091Z","iopub.status.idle":"2025-04-13T16:38:35.265873Z","shell.execute_reply.started":"2025-04-13T16:38:11.455066Z","shell.execute_reply":"2025-04-13T16:38:35.26512Z"},"id":"k27tKzE6qIn0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67b3802a-a702-4dcb-8a6c-50df9b0e96bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-05-07 03:07:46.776405: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-05-07 03:07:46.794249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1746587266.815918    1694 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1746587266.822929    1694 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-05-07 03:07:46.844741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[2025-05-07 03:07:52,374] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","2025-05-07 03:07:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n","2025-05-07 03:07:54 - INFO - __main__ - Model parameters ModelConfig(model_name_or_path='stpete2/Qwen2.5-0.5B-gsm8k-sft', model_revision='main', torch_dtype='bfloat16', trust_remote_code=False, attn_implementation='eager', use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, lora_task_type='CAUSAL_LM', use_rslora=False, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\n","2025-05-07 03:07:54 - INFO - __main__ - Script parameters GRPOScriptArguments(dataset_name='stpete2/openai-gsm8k-part', dataset_config=None, dataset_train_split='train', dataset_test_split='test', gradient_checkpointing_use_reentrant=False, ignore_bias_buffers=False, reward_funcs=['accuracy', 'format', 'tag_count'], cosine_min_value_wrong=0.0, cosine_max_value_wrong=-0.5, cosine_min_value_correct=0.5, cosine_max_value_correct=1.0, cosine_max_len=1000, repetition_n_grams=3, repetition_max_penalty=-1.0, code_language='python')\n","2025-05-07 03:07:54 - INFO - __main__ - Training parameters GRPOConfig(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","allocation=True,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=False,\n","batch_eval_metrics=False,\n","benchmarks=[],\n","beta=0.04,\n","bf16=True,\n","bf16_full_eval=False,\n","callbacks=[],\n","chat_template=None,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=False,\n","do_predict=False,\n","do_train=False,\n","ds3_gather_for_generation=True,\n","epsilon=0.2,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=IntervalStrategy.NO,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=2,\n","gradient_checkpointing=True,\n","gradient_checkpointing_kwargs={'use_reentrant': False},\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=Qwen2.5-0.5B-gsm8k-cppo,\n","hub_model_revision=main,\n","hub_private_repo=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-06,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_completions=True,\n","log_level=info,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=data/Qwen2.5-0.5B-gsm8k-cppo/runs/May07_03-07-54_3e9c74853894,\n","logging_first_step=True,\n","logging_nan_inf_filter=True,\n","logging_steps=1,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=SchedulerType.COSINE,\n","max_completion_length=512,\n","max_grad_norm=1.0,\n","max_prompt_length=256,\n","max_steps=-1,\n","metric=smallest,\n","metric_for_best_model=None,\n","model_init_kwargs=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_generations=8,\n","num_iterations=1,\n","num_train_epochs=1,\n","optim=OptimizerNames.ADAMW_TORCH,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=data/Qwen2.5-0.5B-gsm8k-cppo,\n","overwrite_hub_revision=False,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=8,\n","prediction_loss_only=False,\n","pruning=0.75,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_revision=False,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","ref_model_mixup_alpha=0.6,\n","ref_model_sync_steps=512,\n","remove_unused_columns=False,\n","report_to=['wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","reward_weights=[1.0, 1.0, 1.0],\n","run_name=data/Qwen2.5-0.5B-gsm8k-cppo,\n","sample_num=0,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=500,\n","save_strategy=SaveStrategy.EPOCH,\n","save_total_limit=1,\n","seed=42,\n","skip_memory_metrics=True,\n","split_batches=None,\n","sync_ref_model=False,\n","system_prompt=You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n","...\n","</think>\n","<answer>\n","...\n","</answer>\n",",\n","temperature=0.9,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","use_vllm=False,\n","vllm_device=auto,\n","vllm_dtype=auto,\n","vllm_enable_prefix_caching=True,\n","vllm_gpu_memory_utilization=0.9,\n","vllm_guided_decoding_regex=None,\n","vllm_max_model_len=None,\n","wandb_entity=None,\n","wandb_name=None,\n","wandb_project=None,\n","warmup_ratio=0.1,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","README.md: 100% 55.0/55.0 [00:00<00:00, 433kB/s]\n","Generating dataset openai-gsm8k-part (/root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0)\n","2025-05-07 03:07:56 - INFO - datasets.builder - Generating dataset openai-gsm8k-part (/root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0)\n","Downloading and preparing dataset openai-gsm8k-part/default to /root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0...\n","2025-05-07 03:07:56 - INFO - datasets.builder - Downloading and preparing dataset openai-gsm8k-part/default to /root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0...\n","gsm8k_split_0.parquet: 100% 130k/130k [00:00<00:00, 5.03MB/s]\n","Downloading took 0.0 min\n","2025-05-07 03:07:56 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","2025-05-07 03:07:56 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Generating train split\n","2025-05-07 03:07:56 - INFO - datasets.builder - Generating train split\n","Generating train split: 100% 400/400 [00:00<00:00, 12721.67 examples/s]\n","All the splits matched successfully.\n","2025-05-07 03:07:57 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset openai-gsm8k-part downloaded and prepared to /root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0. Subsequent calls will reuse this data.\n","2025-05-07 03:07:57 - INFO - datasets.builder - Dataset openai-gsm8k-part downloaded and prepared to /root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0. Subsequent calls will reuse this data.\n","tokenizer_config.json: 100% 4.69k/4.69k [00:00<00:00, 26.5MB/s]\n","vocab.json: 100% 2.78M/2.78M [00:00<00:00, 7.77MB/s]\n","merges.txt: 100% 1.67M/1.67M [00:00<00:00, 7.69MB/s]\n","tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 52.7MB/s]\n","added_tokens.json: 100% 605/605 [00:00<00:00, 4.88MB/s]\n","special_tokens_map.json: 100% 616/616 [00:00<00:00, 5.04MB/s]\n","chat_template.jinja: 100% 2.43k/2.43k [00:00<00:00, 22.6MB/s]\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/vocab.json\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/merges.txt\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/tokenizer.json\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/added_tokens.json\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2050] 2025-05-07 03:07:59,419 >> loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--stpete2--Qwen2.5-0.5B-gsm8k-sft/snapshots/700386ce2a6581dce388b449f47a6dc8b2acfc17/chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-05-07 03:07:59,906 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Map:   0% 0/400 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0/cache-5ccd70d4ed83e400.arrow\n","2025-05-07 03:07:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/stpete2___openai-gsm8k-part/default/0.0.0/a196359a8dfc66a7c2bb6d3433bb8e1e3d9dbbc0/cache-5ccd70d4ed83e400.arrow\n","Map: 100% 400/400 [00:00<00:00, 18587.24 examples/s]\n","2025-05-07 03:07:59 - INFO - __main__ - *** Initializing model kwargs ***\n","config.json: 100% 750/750 [00:00<00:00, 6.91MB/s]\n","adapter_config.json: 100% 712/712 [00:00<00:00, 7.14MB/s]\n","config.json: 100% 681/681 [00:00<00:00, 6.35MB/s]\n","[INFO|configuration_utils.py:699] 2025-05-07 03:08:00,587 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n","[INFO|configuration_utils.py:771] 2025-05-07 03:08:00,589 >> Model config Qwen2Config {\n","  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151643,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 896,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4864,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 24,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 14,\n","  \"num_hidden_layers\": 24,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": 32768,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": false,\n","  \"use_mrope\": false,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","model.safetensors: 100% 988M/988M [00:03<00:00, 271MB/s]\n","[INFO|modeling_utils.py:3982] 2025-05-07 03:08:04,381 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/model.safetensors\n","[INFO|modeling_utils.py:1633] 2025-05-07 03:08:04,429 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-05-07 03:08:04,431 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151643,\n","  \"use_cache\": false\n","}\n","\n","[WARNING|logging.py:329] 2025-05-07 03:08:04,442 >> Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n","[INFO|modeling_utils.py:4970] 2025-05-07 03:08:04,506 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-05-07 03:08:04,506 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n","generation_config.json: 100% 138/138 [00:00<00:00, 1.36MB/s]\n","[INFO|configuration_utils.py:1095] 2025-05-07 03:08:04,762 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-05-07 03:08:04,762 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151643,\n","  \"max_new_tokens\": 2048\n","}\n","\n","adapter_model.safetensors: 100% 1.09M/1.09M [00:00<00:00, 10.9MB/s]\n","[INFO|trainer.py:746] 2025-05-07 03:08:06,517 >> Using auto half precision backend\n","2025-05-07 03:08:06 - INFO - __main__ - *** Train ***\n","[INFO|trainer.py:2405] 2025-05-07 03:08:07,160 >> ***** Running training *****\n","[INFO|trainer.py:2406] 2025-05-07 03:08:07,160 >>   Num examples = 400\n","[INFO|trainer.py:2407] 2025-05-07 03:08:07,160 >>   Num Epochs = 1\n","[INFO|trainer.py:2408] 2025-05-07 03:08:07,160 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:2411] 2025-05-07 03:08:07,160 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2412] 2025-05-07 03:08:07,160 >>   Gradient Accumulation steps = 2\n","[INFO|trainer.py:2413] 2025-05-07 03:08:07,160 >>   Total optimization steps = 50\n","[INFO|trainer.py:2414] 2025-05-07 03:08:07,162 >>   Number of trainable parameters = 0\n","[INFO|integration_utils.py:817] 2025-05-07 03:08:07,163 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ./wandb/wandb/ wasn't writable, using system temp directory\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstpeteishii\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/wandb/run-20250507_030807-80omi270\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdata/Qwen2.5-0.5B-gsm8k-cppo\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/stpeteishii/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/stpeteishii/huggingface/runs/80omi270\u001b[0m\n","  0% 0/50 [00:00<?, ?it/s]Begin Training\n","[WARNING|logging.py:329] 2025-05-07 03:08:08,228 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","Begin Training\n","{'loss': -0.1203, 'grad_norm': 0.0, 'learning_rate': 4e-07, 'pruning_rate': 0.75, 'completion_length': 284.875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.0007121476082829759, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0007121468661352992, 'kl': 0.0014549869229085743, 'clip_ratio': 0.0, 'epoch': 0.02}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8e-07, 'pruning_rate': 0.75, 'completion_length': 335.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': -0.00012075024278601632, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00012075024278601632, 'kl': 0.0011228578223381191, 'clip_ratio': 0.0, 'epoch': 0.04}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.2e-06, 'pruning_rate': 0.75, 'completion_length': 306.1875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.25, 'reward': 0.25, 'reward_std': 0.0, 'second_item': 0.00021771483443444595, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00021771483443444595, 'kl': 0.000461537012597546, 'clip_ratio': 0.0, 'epoch': 0.06}\n","{'loss': 0.1758, 'grad_norm': 0.0, 'learning_rate': 1.6e-06, 'pruning_rate': 0.75, 'completion_length': 430.75, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.140625, 'reward': 0.140625, 'reward_std': 0.19887378066778183, 'second_item': -0.00010821042087627575, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00010821118485182524, 'kl': 0.0009553743875585496, 'clip_ratio': 0.0, 'epoch': 0.08}\n","{'loss': -0.3131, 'grad_norm': 0.0, 'learning_rate': 2e-06, 'pruning_rate': 0.75, 'completion_length': 330.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': 0.00024259581550722942, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0002425957572995685, 'kl': 0.0010885776719078422, 'clip_ratio': 0.0, 'epoch': 0.1}\n","{'loss': -0.0831, 'grad_norm': 0.0, 'learning_rate': 1.997564050259824e-06, 'pruning_rate': 0.75, 'completion_length': 319.375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.09375, 'reward': 0.09375, 'reward_std': 0.13258251547813416, 'second_item': 0.0003274339615018107, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0003274343616794795, 'kl': 0.001190765993669629, 'clip_ratio': 0.0, 'epoch': 0.12}\n","{'loss': -0.0723, 'grad_norm': 0.0, 'learning_rate': 1.99026806874157e-06, 'pruning_rate': 0.75, 'completion_length': 270.125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.00021641890634782612, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00021641785860992968, 'kl': 0.0006383057043422014, 'clip_ratio': 0.0, 'epoch': 0.14}\n","{'loss': 0.1244, 'grad_norm': 0.0, 'learning_rate': 1.9781476007338054e-06, 'pruning_rate': 0.75, 'completion_length': 338.3125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.03125, 'reward': 0.03125, 'reward_std': 0.04419417306780815, 'second_item': 0.0007881456112954766, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0007881455530878156, 'kl': 0.0015724855184089392, 'clip_ratio': 0.0, 'epoch': 0.16}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.9612616959383188e-06, 'pruning_rate': 0.75, 'completion_length': 379.75, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.0, 'second_item': -0.00020265970670152456, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00020265970670152456, 'kl': 0.0009358297102153301, 'clip_ratio': 0.0, 'epoch': 0.18}\n","{'loss': -0.006, 'grad_norm': 0.0, 'learning_rate': 1.9396926207859082e-06, 'pruning_rate': 0.75, 'completion_length': 274.6875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.00011874453775817528, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00011874389747390524, 'kl': 0.0008191934030037373, 'clip_ratio': 0.0, 'epoch': 0.2}\n","{'loss': -0.1376, 'grad_norm': 0.0, 'learning_rate': 1.9135454576426007e-06, 'pruning_rate': 0.75, 'completion_length': 279.5625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.0009740216773934662, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0009740210953168571, 'kl': 0.0010310511570423841, 'clip_ratio': 0.0, 'epoch': 0.22}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.8829475928589268e-06, 'pruning_rate': 0.75, 'completion_length': 344.0625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.03125, 'reward': 0.03125, 'reward_std': 0.04419417306780815, 'second_item': -0.00046296629807329737, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00046296557411551476, 'kl': 0.0011170644720550627, 'clip_ratio': 0.0, 'epoch': 0.24}\n","{'loss': -0.1665, 'grad_norm': 0.0, 'learning_rate': 1.8480480961564257e-06, 'pruning_rate': 0.75, 'completion_length': 335.125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': -7.723853923380375e-05, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -7.723818998783827e-05, 'kl': 0.001355627493467182, 'clip_ratio': 0.0, 'epoch': 0.26}\n","{'loss': -0.0154, 'grad_norm': 0.0, 'learning_rate': 1.8090169943749474e-06, 'pruning_rate': 0.75, 'completion_length': 334.5625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.1875, 'reward': 0.1875, 'reward_std': 0.2651650384068489, 'second_item': 0.0008935581718105823, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0008935581427067518, 'kl': 0.0011000992089975625, 'clip_ratio': 0.0, 'epoch': 0.28}\n","{'loss': -0.0581, 'grad_norm': 0.0, 'learning_rate': 1.766044443118978e-06, 'pruning_rate': 0.75, 'completion_length': 304.3125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.15625, 'reward': 0.15625, 'reward_std': 0.22097086533904076, 'second_item': 0.00046222368564485805, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00046222307719290257, 'kl': 0.002259429107652977, 'clip_ratio': 0.0, 'epoch': 0.3}\n","{'loss': 0.0001, 'grad_norm': 0.0, 'learning_rate': 1.719339800338651e-06, 'pruning_rate': 0.75, 'completion_length': 304.4375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.0001809658915590262, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00018096504209097475, 'kl': 0.0014795242459513247, 'clip_ratio': 0.0, 'epoch': 0.32}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.669130606358858e-06, 'pruning_rate': 0.75, 'completion_length': 345.125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.078125, 'reward': 0.078125, 'reward_std': 0.11048543453216553, 'second_item': 0.0004527814708126243, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0004527807468548417, 'kl': 0.0007997929351404309, 'clip_ratio': 0.0, 'epoch': 0.34}\n","{'loss': -0.2363, 'grad_norm': 0.0, 'learning_rate': 1.615661475325658e-06, 'pruning_rate': 0.75, 'completion_length': 318.875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.1875, 'reward': 0.1875, 'reward_std': 0.2651650384068489, 'second_item': -0.00012114153651054949, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00012113875709474087, 'kl': 0.001301413867622614, 'clip_ratio': 0.0, 'epoch': 0.36}\n","{'loss': 0.0515, 'grad_norm': 0.0, 'learning_rate': 1.5591929034707466e-06, 'pruning_rate': 0.75, 'completion_length': 333.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': 0.0014363233494805172, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0014363217633217573, 'kl': 0.0007492836157325655, 'clip_ratio': 0.0, 'epoch': 0.38}\n","{'loss': 0.0001, 'grad_norm': 0.0, 'learning_rate': 1.5e-06, 'pruning_rate': 0.75, 'completion_length': 312.75, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.0007243573199957609, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0007243573199957609, 'kl': 0.0014591505751013756, 'clip_ratio': 0.0, 'epoch': 0.4}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.4383711467890773e-06, 'pruning_rate': 0.75, 'completion_length': 294.3125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.00044178517418913543, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00044178517418913543, 'kl': 0.0007135632622521371, 'clip_ratio': 0.0, 'epoch': 0.42}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.374606593415912e-06, 'pruning_rate': 0.75, 'completion_length': 276.0625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 2.743327058851719e-06, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 2.743327058851719e-06, 'kl': 0.0015658671036362648, 'clip_ratio': 0.0, 'epoch': 0.44}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.3090169943749473e-06, 'pruning_rate': 0.75, 'completion_length': 298.3125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': -0.00016409547970397398, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00016409547970397398, 'kl': 0.0013033299474045634, 'clip_ratio': 0.0, 'epoch': 0.46}\n","{'loss': -0.0492, 'grad_norm': 0.0, 'learning_rate': 1.2419218955996676e-06, 'pruning_rate': 0.75, 'completion_length': 325.75, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.00025984147214330733, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0002598412393126637, 'kl': 0.0007112802122719586, 'clip_ratio': 0.0, 'epoch': 0.48}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.1736481776669305e-06, 'pruning_rate': 0.75, 'completion_length': 351.1875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.0006525287753902376, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0006525287753902376, 'kl': 0.0007768263749312609, 'clip_ratio': 0.0, 'epoch': 0.5}\n","{'loss': -0.221, 'grad_norm': 0.0, 'learning_rate': 1.1045284632676535e-06, 'pruning_rate': 0.75, 'completion_length': 333.375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.1875, 'reward': 0.1875, 'reward_std': 0.0883883461356163, 'second_item': -0.00016668674652464688, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.0001666866010054946, 'kl': 0.000860460801050067, 'clip_ratio': 0.0, 'epoch': 0.52}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.034899496702501e-06, 'pruning_rate': 0.75, 'completion_length': 274.6875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.0007349543739110231, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0007349543739110231, 'kl': 0.0012810309999622405, 'clip_ratio': 0.0, 'epoch': 0.54}\n","{'loss': -0.1825, 'grad_norm': 0.0, 'learning_rate': 9.651005032974993e-07, 'pruning_rate': 0.75, 'completion_length': 261.0, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.078125, 'reward': 0.078125, 'reward_std': 0.11048543266952038, 'second_item': 0.0002504053190932609, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0002504047006368637, 'kl': 0.0008477247611153871, 'clip_ratio': 0.0, 'epoch': 0.56}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.954715367323466e-07, 'pruning_rate': 0.75, 'completion_length': 308.25, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.0001919724472827511, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00019197294022887945, 'kl': 0.0008943697321228683, 'clip_ratio': 0.0, 'epoch': 0.58}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.263518223330696e-07, 'pruning_rate': 0.75, 'completion_length': 369.25, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.00030060143035370857, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00030060143035370857, 'kl': 0.0010684115113690495, 'clip_ratio': 0.0, 'epoch': 0.6}\n","{'loss': -0.2068, 'grad_norm': 0.0, 'learning_rate': 7.580781044003324e-07, 'pruning_rate': 0.75, 'completion_length': 353.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': -0.0002784255484584719, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.0002784249372780323, 'kl': 0.0013192175974836573, 'clip_ratio': 0.0, 'epoch': 0.62}\n","{'loss': -0.0821, 'grad_norm': 0.0, 'learning_rate': 6.909830056250526e-07, 'pruning_rate': 0.75, 'completion_length': 398.875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': -0.00038389948531403206, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00038389928522519767, 'kl': 0.0006559961475431919, 'clip_ratio': 0.0, 'epoch': 0.64}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.253934065840879e-07, 'pruning_rate': 0.75, 'completion_length': 342.625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.0003148570249322802, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0003148585092276335, 'kl': 0.0010042793001048267, 'clip_ratio': 0.0, 'epoch': 0.66}\n","{'loss': 0.26, 'grad_norm': 0.0, 'learning_rate': 5.616288532109224e-07, 'pruning_rate': 0.75, 'completion_length': 239.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': 9.324110578745604e-05, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 9.323970880359411e-05, 'kl': 0.0010555145563557744, 'clip_ratio': 0.0, 'epoch': 0.68}\n","{'loss': -0.2174, 'grad_norm': 0.0, 'learning_rate': 5.000000000000002e-07, 'pruning_rate': 0.75, 'completion_length': 390.1875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': 0.00037686110863432987, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0003768596798181534, 'kl': 0.0017241486348211765, 'clip_ratio': 0.0, 'epoch': 0.7}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.408070965292533e-07, 'pruning_rate': 0.75, 'completion_length': 330.0, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': -8.067319504334591e-05, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -8.067406452028081e-05, 'kl': 0.0007499424100387841, 'clip_ratio': 0.0, 'epoch': 0.72}\n","{'loss': 0.0001, 'grad_norm': 0.0, 'learning_rate': 3.843385246743417e-07, 'pruning_rate': 0.75, 'completion_length': 297.625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': 0.00023476802380173467, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0002347668050788343, 'kl': 0.001325578021351248, 'clip_ratio': 0.0, 'epoch': 0.74}\n","{'loss': -0.2954, 'grad_norm': 0.0, 'learning_rate': 3.308693936411421e-07, 'pruning_rate': 0.75, 'completion_length': 280.5625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': 0.001968827622476965, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0019688286702148616, 'kl': 0.002025329478783533, 'clip_ratio': 0.0, 'epoch': 0.76}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.8066019966134904e-07, 'pruning_rate': 0.75, 'completion_length': 392.125, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.00030094467365415767, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00030094467365415767, 'kl': 0.000999200507067144, 'clip_ratio': 0.0, 'epoch': 0.78}\n","{'loss': -0.1686, 'grad_norm': 0.0, 'learning_rate': 2.339555568810221e-07, 'pruning_rate': 0.75, 'completion_length': 367.9375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.015625, 'reward': 0.015625, 'reward_std': 0.022097086533904076, 'second_item': 0.0009080641902983189, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0009080649469979107, 'kl': 0.0012229555286467075, 'clip_ratio': 0.0, 'epoch': 0.8}\n","{'loss': 0.0001, 'grad_norm': 0.0, 'learning_rate': 1.9098300562505264e-07, 'pruning_rate': 0.75, 'completion_length': 303.9375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.00046277967339847237, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00046277967339847237, 'kl': 0.0022935184533707798, 'clip_ratio': 0.0, 'epoch': 0.82}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.5195190384357404e-07, 'pruning_rate': 0.75, 'completion_length': 370.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.0003582655517675448, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0003582655517675448, 'kl': 0.0010678853141143918, 'clip_ratio': 0.0, 'epoch': 0.84}\n","{'loss': -0.1677, 'grad_norm': 0.0, 'learning_rate': 1.1705240714107301e-07, 'pruning_rate': 0.75, 'completion_length': 337.6875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0625, 'reward': 0.0625, 'reward_std': 0.0883883461356163, 'second_item': -0.0001828466452025168, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00018284673251400818, 'kl': 0.0013772622914984822, 'clip_ratio': 0.0, 'epoch': 0.86}\n","{'loss': -0.1382, 'grad_norm': 0.0, 'learning_rate': 8.645454235739902e-08, 'pruning_rate': 0.75, 'completion_length': 289.9375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.09375, 'reward': 0.09375, 'reward_std': 0.13258251547813416, 'second_item': 0.00047601156984455884, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0004760119190905243, 'kl': 0.0017375568277202547, 'clip_ratio': 0.0, 'epoch': 0.88}\n","{'loss': 0.0001, 'grad_norm': 0.0, 'learning_rate': 6.030737921409168e-08, 'pruning_rate': 0.75, 'completion_length': 323.625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.00021578426094492897, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.00021578426094492897, 'kl': 0.0014163716696202755, 'clip_ratio': 0.0, 'epoch': 0.9}\n","{'loss': -0.0409, 'grad_norm': 0.0, 'learning_rate': 3.87383040616811e-08, 'pruning_rate': 0.75, 'completion_length': 303.875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.1767766922712326, 'second_item': 0.0015273591234290507, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0015273566823452711, 'kl': 0.002300575200933963, 'clip_ratio': 0.0, 'epoch': 0.92}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.185239926619431e-08, 'pruning_rate': 0.75, 'completion_length': 392.0625, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.125, 'reward': 0.125, 'reward_std': 0.0, 'second_item': 0.0002583027471700916, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0002583027471700916, 'kl': 0.0007402738556265831, 'clip_ratio': 0.0, 'epoch': 0.94}\n","{'loss': 0.0837, 'grad_norm': 0.0, 'learning_rate': 9.731931258429638e-09, 'pruning_rate': 0.75, 'completion_length': 396.875, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.140625, 'reward': 0.140625, 'reward_std': 0.19887377880513668, 'second_item': 0.0002115791867254302, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0002115796087309718, 'kl': 0.0006395058589987457, 'clip_ratio': 0.0, 'epoch': 0.96}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.435949740175802e-09, 'pruning_rate': 0.75, 'completion_length': 328.4375, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': -0.00013846068031853065, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': -0.00013846068031853065, 'kl': 0.0005797682824777439, 'clip_ratio': 0.0, 'epoch': 0.98}\n","{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'pruning_rate': 0.75, 'completion_length': 301.5, 'rewards/accuracy_reward': 0.0, 'rewards/format_reward': 0.0, 'rewards/tag_count_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'second_item': 0.0011838051723316312, 'first_item': 0.0, 'first_item_div_second_item': 0.0, 'total_sum': 0.0011838051723316312, 'kl': 0.0013023019419051707, 'clip_ratio': 0.0, 'epoch': 1.0}\n","100% 50/50 [46:41<00:00, 56.01s/it][INFO|trainer.py:3942] 2025-05-07 03:54:49,394 >> Saving model checkpoint to data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50\n","[INFO|configuration_utils.py:909] 2025-05-07 03:54:49,396 >> Configuration saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/generation_config.json\n","[INFO|modeling_utils.py:2825] 2025-05-07 03:54:49,396 >> Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n","[INFO|configuration_utils.py:699] 2025-05-07 03:54:49,675 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n","[INFO|configuration_utils.py:771] 2025-05-07 03:54:49,676 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151643,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 896,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4864,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 24,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 14,\n","  \"num_hidden_layers\": 24,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": 32768,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"use_mrope\": false,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|modeling_utils.py:2831] 2025-05-07 03:54:49,677 >> To match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\n","[INFO|modeling_utils.py:3040] 2025-05-07 03:54:49,691 >> Model weights saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/adapter_model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-05-07 03:54:49,692 >> tokenizer config file saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-05-07 03:54:49,693 >> Special tokens file saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/special_tokens_map.json\n","[INFO|trainer.py:3942] 2025-05-07 03:54:49,920 >> Saving model checkpoint to data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50\n","[INFO|configuration_utils.py:909] 2025-05-07 03:54:49,922 >> Configuration saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/generation_config.json\n","[INFO|modeling_utils.py:2825] 2025-05-07 03:54:49,922 >> Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n","[INFO|configuration_utils.py:699] 2025-05-07 03:54:50,126 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n","[INFO|configuration_utils.py:771] 2025-05-07 03:54:50,127 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151643,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 896,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4864,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 24,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 14,\n","  \"num_hidden_layers\": 24,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": 32768,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"use_mrope\": false,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|modeling_utils.py:2831] 2025-05-07 03:54:50,128 >> To match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\n","[INFO|modeling_utils.py:3040] 2025-05-07 03:54:50,142 >> Model weights saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/adapter_model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-05-07 03:54:50,143 >> tokenizer config file saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-05-07 03:54:50,144 >> Special tokens file saved in data/Qwen2.5-0.5B-gsm8k-cppo/checkpoint-50/special_tokens_map.json\n","[INFO|trainer.py:2657] 2025-05-07 03:54:50,364 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2803.2026, 'train_samples_per_second': 0.143, 'train_steps_per_second': 0.018, 'train_loss': -0.045647083729454606, 'epoch': 1.0}\n","100% 50/50 [46:42<00:00, 56.05s/it]\n","\n","Training + Eval time: 2773.490843418\n","\n","Eval time: 1.0342948750004552\n","\n","Training time: 2772.456548543\n","***** train metrics *****\n","  total_flos               =        0GF\n","  train_loss               =    -0.0456\n","  train_runtime            = 0:46:43.20\n","  train_samples            =        400\n","  train_samples_per_second =      0.143\n","  train_steps_per_second   =      0.018\n","2025-05-07 03:54:50 - INFO - __main__ - *** Save model ***\n","[INFO|trainer.py:3942] 2025-05-07 03:54:50,371 >> Saving model checkpoint to data/Qwen2.5-0.5B-gsm8k-cppo\n","[INFO|configuration_utils.py:909] 2025-05-07 03:54:50,373 >> Configuration saved in data/Qwen2.5-0.5B-gsm8k-cppo/generation_config.json\n","[INFO|modeling_utils.py:2825] 2025-05-07 03:54:50,373 >> Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\n","[INFO|configuration_utils.py:699] 2025-05-07 03:54:50,596 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n","[INFO|configuration_utils.py:771] 2025-05-07 03:54:50,597 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151643,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 896,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4864,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 24,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 14,\n","  \"num_hidden_layers\": 24,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": 32768,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"use_mrope\": false,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|modeling_utils.py:2831] 2025-05-07 03:54:50,598 >> To match the expected format of the PEFT library, all keys of the state dict of adapters will be pre-pended with `base_model.model`.\n","[INFO|modeling_utils.py:3040] 2025-05-07 03:54:50,611 >> Model weights saved in data/Qwen2.5-0.5B-gsm8k-cppo/adapter_model.safetensors\n","[INFO|tokenization_utils_base.py:2500] 2025-05-07 03:54:50,612 >> tokenizer config file saved in data/Qwen2.5-0.5B-gsm8k-cppo/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2509] 2025-05-07 03:54:50,612 >> Special tokens file saved in data/Qwen2.5-0.5B-gsm8k-cppo/special_tokens_map.json\n","2025-05-07 03:54:50 - INFO - __main__ - Model saved to data/Qwen2.5-0.5B-gsm8k-cppo\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n","[INFO|configuration_utils.py:423] 2025-05-07 03:54:50,832 >> Configuration saved in data/Qwen2.5-0.5B-gsm8k-cppo/config.json\n"]}],"execution_count":6},{"cell_type":"code","source":"import shutil\nfrom google.colab import files\nfolder_path = \"data\"\nzip_filename = \"data.zip\"\nshutil.make_archive(folder_path, 'zip', folder_path)\nfiles.download(zip_filename)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"3bWigcJ_5i4j","outputId":"a563ad14-aa1d-42c1-8303-cbcb6dea7e18"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_42ba9fd7-70dc-41c1-93f2-c59d80a581c0\", \"data.zip\", 9534087)"]},"metadata":{}}],"execution_count":7}]}